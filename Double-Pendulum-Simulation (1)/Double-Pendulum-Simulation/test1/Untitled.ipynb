{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6ad492-a3ae-40dc-9cfa-a4deb83c8c59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Integrating: 100%|██████████| 360001/360001 [23:25<00:00, 256.21chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to pendulum_data360000.npz\n",
      "Loaded Data:\n",
      "{'Time': (360000001,), 'Theta1': (360000001,), 'Theta2': (360000001,), 'X1': (360000001,), 'Y1': (360000001,), 'X2': (360000001,), 'Y2': (360000001,), 'Energy': (360000001,)}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Pendulum rod lengths (m), bob masses (kg).\n",
    "L1, L2 = 1, 1\n",
    "m1, m2 = 1, 1\n",
    "# The gravitational acceleration (m.s-2).\n",
    "g = 9.81\n",
    "print(\"starting...\")\n",
    "\n",
    "def deriv(y, t, L1, L2, m1, m2):\n",
    "    \"\"\"Return the first derivatives of y = theta1, z1, theta2, z2.\"\"\"\n",
    "    theta1, z1, theta2, z2 = y\n",
    "    c, s = np.cos(theta1-theta2), np.sin(theta1-theta2)\n",
    "    theta1dot = z1\n",
    "    z1dot = (m2*g*np.sin(theta2)*c - m2*s*(L1*z1**2*c + L2*z2**2) -\n",
    "             (m1+m2)*g*np.sin(theta1)) / L1 / (m1 + m2*s**2)\n",
    "    theta2dot = z2\n",
    "    z2dot = ((m1+m2)*(L1*z1**2*s - g*np.sin(theta2) + g*np.sin(theta1)*c) + \n",
    "             m2*L2*z2**2*s*c) / L2 / (m1 + m2*s**2)\n",
    "    return theta1dot, z1dot, theta2dot, z2dot\n",
    "\n",
    "def calc_E(y):\n",
    "    \"\"\"Return the total energy of the system.\"\"\"\n",
    "    th1, th1d, th2, th2d = y.T\n",
    "    V = -(m1+m2)*L1*g*np.cos(th1) - m2*L2*g*np.cos(th2)\n",
    "    T = 0.5*m1*(L1*th1d)**2 + 0.5*m2*((L1*th1d)**2 + (L2*th2d)**2 +\n",
    "            2*L1*L2*th1d*th2d*np.cos(th1-th2))\n",
    "    return T + V\n",
    "\n",
    "# Maximum time, time point spacings, and the time grid (all in s).\n",
    "tmax, dt = 360000, 0.001\n",
    "t = np.arange(0, tmax+dt, dt)\n",
    "# Initial conditions: theta1, dtheta1/dt, theta2, dtheta2/dt.\n",
    "y0 = np.array([3*np.pi/7, 0, 3*np.pi/4, 0])\n",
    "\n",
    "# Break the time array into chunks to use with tqdm\n",
    "chunk_size = 1000\n",
    "chunks = [t[i:i + chunk_size] for i in range(0, len(t), chunk_size)]\n",
    "\n",
    "# Define the file path\n",
    "file_path = f\"pendulum_data{tmax}.npz\"\n",
    "\n",
    "# Initialize storage lists\n",
    "time_data, theta1_data, theta2_data = [], [], []\n",
    "x1_data, y1_data, x2_data, y2_data, energy_data = [], [], [], [], []\n",
    "\n",
    "# Use tqdm to show the progress\n",
    "for chunk in tqdm(chunks, desc=\"Integrating\", unit=\"chunk\"):\n",
    "    # Perform the numerical integration on each chunk\n",
    "    y_chunk = odeint(deriv, y0, chunk, args=(L1, L2, m1, m2))\n",
    "\n",
    "    # Unpack theta and convert to Cartesian coordinates.\n",
    "    theta1, theta2 = y_chunk[:, 0], y_chunk[:, 2]\n",
    "    x1 = L1 * np.sin(theta1)\n",
    "    y1 = -L1 * np.cos(theta1)\n",
    "    x2 = x1 + L2 * np.sin(theta2)\n",
    "    y2 = y1 - L2 * np.cos(theta2)\n",
    "\n",
    "    # Calculate the energy\n",
    "    E = calc_E(y_chunk)\n",
    "\n",
    "    # Store data\n",
    "    time_data.append(chunk)\n",
    "    theta1_data.append(theta1)\n",
    "    theta2_data.append(theta2)\n",
    "    x1_data.append(x1)\n",
    "    y1_data.append(y1)\n",
    "    x2_data.append(x2)\n",
    "    y2_data.append(y2)\n",
    "    energy_data.append(E)\n",
    "\n",
    "    # Update initial conditions for the next chunk\n",
    "    y0 = y_chunk[-1]  # The last value of this chunk is the initial condition for the next chunk\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "np.savez(file_path,\n",
    "         Time=np.concatenate(time_data),\n",
    "         Theta1=np.concatenate(theta1_data),\n",
    "         Theta2=np.concatenate(theta2_data),\n",
    "         X1=np.concatenate(x1_data),\n",
    "         Y1=np.concatenate(y1_data),\n",
    "         X2=np.concatenate(x2_data),\n",
    "         Y2=np.concatenate(y2_data),\n",
    "         Energy=np.concatenate(energy_data))\n",
    "\n",
    "print(f\"Data saved to {file_path}\")\n",
    "\n",
    "# Load and print the saved data (optional)\n",
    "loaded_data = np.load(file_path)\n",
    "print(\"Loaded Data:\")\n",
    "print({key: loaded_data[key].shape for key in loaded_data.keys()})\n",
    "\n",
    "# Convert NPZ data to a Pandas DataFrame\n",
    "df = pd.DataFrame({key: data[key] for key in data.keys()})\n",
    "\n",
    "# Print the first few rows\n",
    "print(tabulate(df, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94930be0-53bc-4930-885e-6b13ecec7325",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'Theta1', 'Theta2', 'X1', 'Y1', 'X2', 'Y2', 'Energy']\n",
      "Time: (30000001,)\n",
      "Theta1: (30000001,)\n",
      "Theta2: (30000001,)\n",
      "X1: (30000001,)\n",
      "Y1: (30000001,)\n",
      "X2: (30000001,)\n",
      "Y2: (30000001,)\n",
      "Energy: (30000001,)\n",
      "    Time    Theta1    Theta2        X1        Y1        X2        Y2    Energy\n",
      "0  0.000  1.346397  2.356194  0.974928 -0.222521  1.682035  0.484586  2.570857\n",
      "1  0.001  1.346392  2.356193  0.974927 -0.222525  1.682034  0.484581  2.570857\n",
      "2  0.002  1.346379  2.356190  0.974924 -0.222538  1.682034  0.484565  2.570857\n",
      "3  0.003  1.346356  2.356185  0.974919 -0.222560  1.682033  0.484540  2.570857\n",
      "4  0.004  1.346325  2.356177  0.974912 -0.222591  1.682031  0.484504  2.570857\n",
      "(30000001, 8)\n",
      "+----+--------+----------+----------+----------+-----------+---------+----------+----------+\n",
      "|    |   Time |   Theta1 |   Theta2 |       X1 |        Y1 |      X2 |       Y2 |   Energy |\n",
      "|----+--------+----------+----------+----------+-----------+---------+----------+----------|\n",
      "|  0 |  0     |  1.3464  |  2.35619 | 0.974928 | -0.222521 | 1.68203 | 0.484586 |  2.57086 |\n",
      "|  1 |  0.001 |  1.34639 |  2.35619 | 0.974927 | -0.222525 | 1.68203 | 0.484581 |  2.57086 |\n",
      "|  2 |  0.002 |  1.34638 |  2.35619 | 0.974924 | -0.222538 | 1.68203 | 0.484565 |  2.57086 |\n",
      "|  3 |  0.003 |  1.34636 |  2.35618 | 0.974919 | -0.22256  | 1.68203 | 0.48454  |  2.57086 |\n",
      "|  4 |  0.004 |  1.34632 |  2.35618 | 0.974912 | -0.222591 | 1.68203 | 0.484504 |  2.57086 |\n",
      "+----+--------+----------+----------+----------+-----------+---------+----------+----------+\n",
      "+----------+--------+-----------+----------+-----------+-----------+----------+----------+----------+\n",
      "|          |   Time |    Theta1 |   Theta2 |        X1 |        Y1 |       X2 |       Y2 |   Energy |\n",
      "|----------+--------+-----------+----------+-----------+-----------+----------+----------+----------|\n",
      "| 29999996 |  30000 | -0.873254 | -957.847 | -0.766423 | -0.642336 | -1.09918 | 0.300677 |  2.54337 |\n",
      "| 29999997 |  30000 | -0.875715 | -957.847 | -0.768001 | -0.640448 | -1.10052 | 0.302647 |  2.54337 |\n",
      "| 29999998 |  30000 | -0.878166 | -957.847 | -0.769569 | -0.638564 | -1.10187 | 0.304609 |  2.54337 |\n",
      "| 29999999 |  30000 | -0.880608 | -957.847 | -0.771126 | -0.636683 | -1.10322 | 0.306563 |  2.54337 |\n",
      "| 30000000 |  30000 | -0.880608 | -957.847 | -0.771126 | -0.636683 | -1.10322 | 0.306563 |  2.54337 |\n",
      "+----------+--------+-----------+----------+-----------+-----------+----------+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# Load the NPZ file\n",
    "file_path = \"pendulum_data30000.npz\"  # Ensure this matches your saved file\n",
    "data = np.load(file_path)\n",
    "\n",
    "print(data.files)  # List all the keys in the NPZ file\n",
    "for key in data.files:\n",
    "    print(f\"{key}: {data[key].shape}\")  # Check the shape of each array\n",
    "\n",
    "# Convert NPZ data to a Pandas DataFrame\n",
    "df = pd.DataFrame({key: data[key] for key in data.keys()})\n",
    "\n",
    "print(df.head())  # This is faster and gives a quick preview\n",
    "print(df.shape)  # Prints number of rows and columns in the DataFrame\n",
    "\n",
    "# Print the first few rows\n",
    "print(tabulate(df.head(), headers='keys', tablefmt='psql'))  # Only print first 10 rows\n",
    "\n",
    "print(tabulate(df.tail(), headers='keys', tablefmt='psql'))  # Only print first 10 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1927c732-9c03-4f37-81cd-2e75ab64a0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data...\n",
      "Data loaded successfully!\n",
      "Standardizing data...\n",
      "Data standardized!\n",
      "Moving data to GPU (if available)...\n",
      "Data moved to cuda\n",
      "Running KMeans clustering with 3 clusters...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m labels, centroids\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Run KMeans\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m clusters, _ \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_data_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# GPU-accelerated PCA (using torch.pca_lowrank)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming PCA for dimensionality reduction...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m, in \u001b[0;36mkmeans_torch\u001b[0;34m(X, n_clusters, max_iter)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkmeans_torch\u001b[39m(X, n_clusters, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning KMeans clustering with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m clusters...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m     n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     32\u001b[0m     centroids \u001b[38;5;241m=\u001b[39m X[torch\u001b[38;5;241m.\u001b[39mrandperm(n_samples)[:n_clusters]]\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load data from .npz file\n",
    "print(\"Loading data...\")\n",
    "data = np.load(file_path)\n",
    "data_key = list(data.keys())[0]  # Assuming data is stored under the first key\n",
    "df_cpu = data[data_key]\n",
    "print(\"Data loaded successfully!\")\n",
    "\n",
    "# Standardization on CPU\n",
    "print(\"Standardizing data...\")\n",
    "mean = np.mean(df_cpu, axis=0)\n",
    "std = np.std(df_cpu, axis=0)\n",
    "scaled_data_cpu = (df_cpu - mean) / std\n",
    "print(\"Data standardized!\")\n",
    "\n",
    "# Convert to PyTorch tensor and move to GPU if available\n",
    "print(\"Moving data to GPU (if available)...\")\n",
    "scaled_data_gpu = torch.tensor(scaled_data_cpu, dtype=torch.float32, device=device)\n",
    "print(\"Data moved to\", device)\n",
    "\n",
    "# GPU-accelerated KMeans (PyTorch implementation)\n",
    "def kmeans_torch(X, n_clusters, max_iter=100):\n",
    "    print(f\"Running KMeans clustering with {n_clusters} clusters...\")\n",
    "    n_samples, n_features = X.shape\n",
    "    centroids = X[torch.randperm(n_samples)[:n_clusters]]\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        distances = torch.cdist(X, centroids)\n",
    "        labels = distances.argmin(dim=1)\n",
    "        new_centroids = torch.stack([X[labels == i].mean(dim=0) for i in range(n_clusters)])\n",
    "        \n",
    "        if torch.all(centroids == new_centroids):\n",
    "            print(f\"KMeans converged at iteration {i + 1}\")\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    print(\"KMeans clustering completed!\")\n",
    "    return labels, centroids\n",
    "\n",
    "# Run KMeans\n",
    "clusters, _ = kmeans_torch(scaled_data_gpu, n_clusters=3)\n",
    "\n",
    "# GPU-accelerated PCA (using torch.pca_lowrank)\n",
    "print(\"Performing PCA for dimensionality reduction...\")\n",
    "U, S, V = torch.pca_lowrank(scaled_data_gpu, q=2)\n",
    "reduced_data_gpu = scaled_data_gpu @ V[:, :2]\n",
    "print(\"PCA completed!\")\n",
    "\n",
    "# Convert back to CPU for visualization\n",
    "print(\"Converting PCA results to CPU...\")\n",
    "reduced_data_np = reduced_data_gpu.cpu().numpy()\n",
    "clusters_np = clusters.cpu().numpy()\n",
    "print(\"Conversion complete!\")\n",
    "\n",
    "# Scatter plot of PCA results\n",
    "print(\"Plotting PCA results...\")\n",
    "plt.scatter(reduced_data_np[:, 0], reduced_data_np[:, 1], c=clusters_np, cmap=\"viridis\", alpha=0.7)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PyTorch-Accelerated PCA Projection of Clusters\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n",
    "print(\"PCA plot generated!\")\n",
    "\n",
    "print(\"All tasks completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c86a0-d076-4b54-8f03-27a522422f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPS",
   "language": "python",
   "name": "dps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
